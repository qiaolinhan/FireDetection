{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients for vectorized code\n",
    "https://www.youtube.com/watch?v=d14TUNcbn1k&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=5&t=2253s\n",
    "\n",
    "Input: $x, y$; Output: $z$. (Locally)  \n",
    "Local gradient: $\\frac{\\partial z}{\\partial x}$, $\\frac{\\partial z}{\\partial y}$  \n",
    "Gradients: $\\frac{\\partial L}{\\partial z}$  \n",
    "Then there is $$\\frac{\\partial L}{\\partial x}=\\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial x}$$  \n",
    "\n",
    "A vectorized example:\n",
    "\\begin{align}\n",
    "f(x, W)=\\Vert W\\cdot x\\Vert ^2 = \\sum _{i=1} ^n(W\\cdot x)_i^2\n",
    "\\end{align}\n",
    "Where $x\\in \\hbox{R}^n, W\\in \\hbox^{n\\times n}$.  \n",
    "Then\n",
    "\\begin{align}\n",
    "q = W\\cdot x &= \n",
    "\\begin{pmatrix}\n",
    "W_{1, 1}x_1+\\cdots +W_{1, n}x_n\\\\\n",
    "\\vdots \\\\\n",
    "W_{n, 1}x_1+\\cdots +W_{n, n}x_n\\\\\n",
    "\\end{pmatrix}\\\\\n",
    "f(q) = \\Vert q \\Vert^2 &= q_1^2+\\cdots + q_n^2\\\\\n",
    "\\frac{\\partial f}{\\partial q_i} &= 2q_i\\\\\n",
    "\\bigtriangledown_q\\ f &= 2q\\\\\n",
    "\\frac{\\partial q_k}{\\partial W_{i, j}} &= 1_{k=i}x_j\\\\\n",
    "\\frac{\\partial f}{\\partial W_{i,j}} \n",
    "= \\sum_k \\frac{f}{q_k} \\frac{\\partial q_k}{W_{i,j}}\n",
    "&= \\sum (2q_k)(1_k= _i x_j) = 2^k q_i x_j \n",
    "\\end{align}\n",
    "* Always check: the gradient with respect to a variable should have the same shape as the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputationalGraph(object):\n",
    "    def forward(inputs):\n",
    "        # 1. [pass inputs to inout gates...]\n",
    "        # 2. forward the computational graph:\n",
    "        for gate i self.graph.nodes_topologically_sorted():\n",
    "            gate.forward()\n",
    "        return loss # final gate in the graph outputs the loss\n",
    "    def backward():\n",
    "        for gate in reversed(self.graph.node_topographically_sorted()):\n",
    "            gate.backward() # little pice of backprop (chain rule applied)  \n",
    "        return input gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplyGate(object):\n",
    "    def forward(x,y):\n",
    "        z = x*y\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return z\n",
    "    def backward(dz):\n",
    "        dx = self.y * dz # [dz/dx * dL/dz]\n",
    "        dy = self.x * dz # [dz/dy * dL/dz]\n",
    "        retun [dx, dy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E.g. for the SVM:\n",
    "# receive W, X\n",
    "scores =  # [f = W*x]\n",
    "margin =  # [max(0,s_j-s_{y_i}+1)]\n",
    "data_loss = \n",
    "reg_loss = \n",
    "loss = data_loss + reg_loss\n",
    "dmargins = \n",
    "dscores = \n",
    "dW = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "&\\text{score:}\\ &f = Wx\\\\\n",
    "&\\text{margins:}\\ &\\text{max}(0, s_j-s_{y_i}+1)\n",
    "\\end{align}\n",
    "## Summary\n",
    "* neural nets will be very large: impactical to write down gradient formula by hand for all parameters\n",
    "* **backpropagation** = recursive application of the chain rule along a computational graph to compute the gradient of all inputs/ parameters/ intermediates\n",
    "* implementations maintain a praph structure, where the nodes implements the **forward()/backward()** API\n",
    "* **forward:** compute result of an operation and save any intermediates needed for gradient computtion in memory\n",
    "* **backward:** apply the chain rule to compute the gradient of the loss function with respect to the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear score function: $f=Wx$  \n",
    "2-layer neural network: $f=W_2\\text{max}(0, W_1x)$  \n",
    "Or 3-layer NN: $f=W_3\\max(0, W_2\\max(0, W1x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-layer NN\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x, y = randn(N, D_in), randn(N, D_out)\n",
    "w1, w2 = randn(D_in, H), randn(H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(200):\n",
    "    h = 1/(1+np.exp(-x.dot(w1)))\n",
    "    y_pred = h.dot(w2)\n",
    "    loss = np.square(y_pred-y).sum()\n",
    "    # print (loss)\n",
    "    \n",
    "    grad_y_pred = 2.0 * (y_pred-y)\n",
    "    grad_w2 = h.T.dot(grad_y_pred)\n",
    "    grad_h = grad_y_pred.dot(w2.T)\n",
    "    grad_w1 = x.T.dot(grad_h*h*(1-h))\n",
    "    \n",
    "    w1 -= 1e-4*grad_w1\n",
    "    w2 -= 1e-4*grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the brain module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def neuron_tick(inputs):\n",
    "        cell_body_sum = np.sum(inputs*self.weights+self.bias)\n",
    "        firing_rate = 1.0/(1.0+math.exp(-cell_body_sum))\n",
    "        return firing_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: 1.0/(1.0+np.exp(-x))\n",
    "x = np.random.randn(3, 1)\n",
    "h1 = f(np.dot(w1, x)+b1)\n",
    "h2 = f(np.sot(w2, h1)+b2)\n",
    "out = np.dot(w3, h2)+b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary2\n",
    "* We arrange neurons into fully-connected layers\n",
    "* The abstraction of a layer has the nice-property that it allows us to use efficient vectorized code (e.g. matrix nultiplies)\n",
    "* NN are not rally nerual"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
